<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python爬虫——记关于爬取网易云音乐评论的折腾]]></title>
    <url>%2Fpython%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%AE%B0%E5%85%B3%E4%BA%8E%E7%88%AC%E5%8F%96%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E8%AF%84%E8%AE%BA%E7%9A%84%E6%8A%98%E8%85%BE.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;为何折腾网易云音乐歌曲评论的爬取，源于博主看到某微信技术公众号推送了一篇关于用python制作词云图的文章。因此想弄个词云图玩玩，奈何手头上没有啥有价值的文本数据，恰巧博主刚好在网易云上听霉霉的新歌，看着几十万的评论，于是想到了把它们爬取下来，就可以做成个词云图，看看大家都在说什么。http://music.163.com/#/song?id=501133611 &emsp;&emsp;二话不说一个右键查看源代码，从上拉到下，一个评论也没发现，动态加载无疑。既然是动态加载，那就找找看加载评论的API接口。F12后点击评论下一页出现了如下图的加载项顺利找到评论的数据包了，其中包括编号、用户名、评论id、评论内容、是否热评、是否点赞、点赞数、时间戳等等。点开它的Headers，发现事情并非这么简单其用的是POST，有两个参数params和encSecKey，很明显这两个参数是经过加密的，每次点击下一页这两个参数都不一样。尝试直接打开这个请求的URL，发现确实什么都没有。要想获取到评论包的数据，必须得加上这两个加密参数。查看Initiator发现这个数据包是由core.js?d123…发起的。打开这个js的链接，一看晕了，几万行的代码还经过混淆处理的。正当我无从下手，发现了这篇文章！https://www.zhihu.com/question/36081767&emsp;&emsp;于是按照文章的方法操作，确实能拿到数据了。正当我满心欢喜的时候发现几十万条评论，只有前面1万+条评论是对的，后面所有的评论都是某两页评论的重复。所以几十万数据下来只有1万+有效数据。网上查一下没人问这个问题，看到的关于爬取网易云音乐评论的文章都是讲解参数加密的。 很纳闷，究竟是哪里出了问题。&emsp;&emsp;第一个想到的是cookie和爬取速度，于是用自己的账号登上去，拿到cookie并将之加入到爬虫脚本中，放慢爬取的速度，重新爬一次。结果还是跟上面的结果一样。&emsp;&emsp;第二个想到的是加密参数，其中offset和limit的值设置都没有问题，而createSecreKye函数中的16长随机字符串用的是固定的16个F，于是重写该函数将16长的字符串改写成随机生成，再次重新爬取，结果还是跟上面的一样。。。 这网易云的反爬有点厉害呀，不服，继续搞。&emsp;&emsp;这次博主直接放弃前面的做法，而是把core.js文件copy下来，用反混淆软件恢复一下，硬着头皮把有关参数加密的js代码一个个找出来，结合PyV8模块，用core.js的代码来生成加密参数。 此部分的代码较多，所以不粘出来，若想要此部分的代码脚本，请邮件联系博主。 测试一下，能拿到数据，非常高兴，于是正式开爬。经过些许的等待，怀着忐忑的心情打开数据文件，一看，又扑街了， 结果还是同上面的一样，存在非常多的重复。 特么开始怀疑人生了。。。到底是哪里出了问题呢。。。既然这样，上selenium吧。 博主用的是Chrome，测试一下如下脚本，立马扑街，没有评论数据。。。1234567891011121314# -*- coding: utf-8 -*-from time import time,sleepfrom selenium import webdriverurl = &apos;http://music.163.com/#/song?id=501133611&apos; driver = webdriver.Chrome(executable_path=&apos;D:\\python\\chromedriver.exe&apos;)driver.get(url)sleep(10)print driver.content.encode(&apos;GB18030&apos;)driver.close()driver.quit() 上网一查，原来网易云音乐的网页端比较特殊，需要进行switch_to_frame操作，至于是哪个frame需要在加载完的代码里查找。脚本如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# -*- coding:utf-8 -*-from selenium import webdriverimport timeimport osimport reimport pymongoimport scrapy# 借助scrapy.Item，方便数据存储class commentItem(scrapy.Item): # define the fields for your item here like: commentCount = scrapy.Field() # 评论内容 userId = scrapy.Field() # 用户id userUrl = scrapy.Field() # 用户URL userName = scrapy.Field() # 用户名 userComment = scrapy.Field() # 评论 commentTime = scrapy.Field() # 评论时间 commentLike = scrapy.Field() # 评论点赞 repFuid = scrapy.Field() # 回复id repFuname = scrapy.Field() # 回复的用户名 repFcm = scrapy.Field() # 回复的评论# 本脚本将爬到的数据存到MongoDB中con = pymongo.MongoClient(&apos;127.0.0.1&apos;, 27017)db = con[&apos;lwymmd&apos;]driver = webdriver.Chrome(executable_path=&apos;D:\\python\\chromedriver.exe&apos;)driver.get(&apos;http://music.163.com/#/song?id=501133611&apos;) # 打开页面os.system(&apos;pause&apos;) # 暂停，等待初始页面加载完成driver.switch_to_frame(&apos;contentFrame&apos;) # 切换frameclickButton = driver.find_element_by_link_text(u&apos;下一页&apos;)item = commentItem()page = 0commentCount = 0while True: try: comments = driver.find_elements_by_class_name(&apos;itm&apos;) # 获取所有评论 for cm in comments[::-1]: # 处理每一条评论 commentCount += 1 userUrl = cm.find_element_by_xpath(&apos;div[@class=&quot;cntwrap&quot;]/div/div/a&apos;).get_attribute(&apos;href&apos;) userId = re.search(u&apos;id=(\d+)&apos;, userUrl).group(1) NandC = cm.find_element_by_xpath(&apos;div[@class=&quot;cntwrap&quot;]/div/div&apos;).text.split(u&apos;：&apos;) userName = NandC[0] userComment = NandC[1] commentTime = cm.find_element_by_xpath(&apos;div[@class=&quot;cntwrap&quot;]/div[@class=&quot;rp&quot;]/div&apos;).text commentLike = cm.find_element_by_xpath(&apos;div[@class=&quot;cntwrap&quot;]/div[@class=&quot;rp&quot;]/a&apos;).text.strip(u&apos;(&apos;).strip(u&apos;)&apos;) if len(commentLike) == 0: commentLike = &apos;0&apos; repFuid = &apos;&apos; repFuname = &apos;&apos; repFcm = &apos;&apos; if cm.find_element_by_xpath(&apos;div[@class=&quot;cntwrap&quot;]/div[2]&apos;).get_attribute(&apos;class&apos;) == u&apos;rp&apos;: pass else: try: repFuid = re.search(u&apos;id=(\d+)&apos;, cm.find_element_by_xpath(&apos;div[@class=&quot;cntwrap&quot;]/div[2]/a&apos;).get_attribute(&apos;href&apos;)).group(1) repFuname = cm.find_element_by_xpath(&apos;div[@class=&quot;cntwrap&quot;]/div[2]/a&apos;).text repFcm = cm.find_element_by_xpath(&apos;div[@class=&quot;cntwrap&quot;]/div[2]&apos;).text.split(u&apos;：&apos;)[1] except: pass item[&apos;commentCount&apos;] = commentCount item[&apos;userId&apos;] = userId item[&apos;userUrl&apos;] = userUrl item[&apos;userName&apos;] = userName item[&apos;userComment&apos;] = userComment item[&apos;commentTime&apos;] = commentTime item[&apos;commentLike&apos;] = commentLike item[&apos;repFuid&apos;] = repFuid item[&apos;repFuname&apos;] = repFuname item[&apos;repFcm&apos;] = repFcm db.comments.insert(dict(item)) page += 1 if page == 1000: # 限制点击的频率 print commentCount/20000 page = 0 time.sleep(60) if u&apos;disabled&apos; in clickButton.get_attribute(&apos;class&apos;): break else: driver.execute_script(&quot;window.scrollTo(0,document.body.scrollHeight)&quot;) clickButton.click() time.sleep(2) except: print &apos;commentCount:&apos;,commentCount print &apos;page:&apos;,page os.system(&apos;pause&apos;) continueprint &apos;commentCount:&apos;,commentCountcon.close() 经过长时间的运行终于爬完了，想着这次应该能成功，打开数据库一看，F…uck…，又是重复。&emsp;&emsp;这下彻底无语了，网易云的反爬这么溜的吗，就是不让你拿到那么多数据。再次不服，一定要找出原因来。&emsp;&emsp;这次直接上手点，一页一页地点，守着屏幕看看究竟能点到哪。其实偷下懒，把上面的脚本稍微改下就不用自己手点了。12345678910111213141516171819202122232425262728# -*- coding:utf-8 -*-from selenium import webdriverimport osdriver = webdriver.Chrome(executable_path=&apos;D:\\python\\chromedriver.exe&apos;)driver.get(&apos;http://music.163.com/#/song?id=501133611&apos;)os.system(&apos;pause&apos;)driver.switch_to_frame(&apos;contentFrame&apos;)clickButton = driver.find_element_by_link_text(u&apos;下一页&apos;)page = 0while True: try: page += 1 if page == 50: # 限制点击的频率 page = 0 time.sleep(60) if u&apos;disabled&apos; in clickButton.get_attribute(&apos;class&apos;): break else: driver.execute_script(&quot;window.scrollTo(0,document.body.scrollHeight)&quot;) clickButton.click() time.sleep(2) except: print &apos;page:&apos;,page os.system(&apos;pause&apos;) continue &emsp;&emsp;随着一页一页往下走，当来到501页时，问题出现了。从501页开始，后面的评论都没有变化，一直重复。&emsp;&emsp;因为有3万多页，所以博主没有继续下去，换了下思路，从最后一页开始往前面点。果然，点了500页后，评论就不变了，开始重复。&emsp;&emsp;于是博主试了一下别的评论过十几万的歌曲，结果也一样。试了下评论几千的歌曲，没发生这个问题。 注：每页的评论数是20 所以结论是：想爬网易云音乐歌曲的评论是可以，但只能爬评论不超过2万的歌曲。评论超过这个数的歌曲无论怎么弄都不行，因为网易云音乐它压根就不给你。 （试了PC客户端跟移动客户端，结果也是如此）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyecharts——连接Echarts与Python]]></title>
    <url>%2Fpyecharts%E2%80%94%E2%80%94%E8%BF%9E%E6%8E%A5Echarts%E4%B8%8EPython.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;上一篇文章python爬虫——豆瓣电影，我们以地区为分类爬取了豆瓣上的电影，本来还要对评分数据进行可视化，但是考虑到篇幅原因，决定另起这篇文章，也对pyecharts模块作一个简单的介绍跟应用。 &emsp;&emsp;pyecharts模块提供了Echarts和Python之间的接口，从而能在Python中通过代码生成Echarts库图表。&emsp;&emsp;ECharts是一个免费的,强大的制图和可视化库，它提供简单的方法添加直观,互动,和高度可定制的图表。它是基于zrender，用纯JavaScript编写的一个全新的轻量级库。&emsp;&emsp;上篇文章最后我们得到了如下的这些数据，其中数据的格式我们也清楚，那就直接进入代码看看怎么处理这些数据以及pyecharts的简单使用。 处理代码如下（代码文件分数据文件存放于同一目录）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# -*- coding:utf-8 -*-import sys reload(sys) sys.setdefaultencoding(&apos;utf-8&apos;)from pyecharts import Lineimport os, reimport copy# 获得当前目录dir = os.path.split(os.path.realpath(__file__))[0]# 将目录中所有文件的文件名加载到 list 中list = os.listdir(dir)# 获取数据文件的文件名并放到 list2parttern = re.compile(&apos;.*?movie.txt&apos;)list2 = []for l in list: if re.search(parttern, l) != None: list2.append(l)parttern = re.compile(&apos;(.*?).movie.txt&apos;)countrylist = [] # 存放地区名称f = &#123;&#125; # 存放打开的文件的文件描述符for l in list2: country = re.search(parttern, l).group(1).decode(&apos;GB18030&apos;) countrylist.append(country) # 将地区名称添加到 countrylist 中 fdir = dir+&apos;\\&apos;+l # 构造数据文件的绝对路径 f[country] = open(fdir) # 打开文件并将其描述符添加到 f 中 f[country].readline() # 并读取首行以便下面的数据处理# 存放所有地区的评分数据 &#123; &#123;地区名称：&#123;评分：数量，评分：数量，评分：数量，......&#125; &#125; ，......&#125;ratedicAll = []# 存放每个地区电影的评分 格式为：&#123;评分：数量，评分：数量，评分：数量，......&#125;ratedic = []# 在这里只是为每个地区生成一个空的 &#123; &#125;for i in range(0,len(countrylist)): ratedic.append(copy.deepcopy(&#123;&#125;)) # 注意此次用的是深复制，如果直接用 &#123;&#125; ，python会以 # 浅复制处理，导致后面的数据重复/出错# Echarts表的设置 # X 轴的设置# X 轴为电影评分，从0开始到10，间隔为0.1x = []n = 0 # 评分i = 0# 为每个地区生成初始的评分字典，&#123;评分：数量，评分：数量，评分：数量，......&#125;# &#123;0:0, 0.1:0, 0.2:0,..........9.8:0, 9.9:0, 10:0&#125;while n &lt;= 10: for i in range(0,len(countrylist)): ratedic[i][n] = 0 x.append(n) n += 0.1 # 这里要进行 round 操作是因为浮点数的关系 # 一直地加 0.1 其结果并不是我们要的 0.2 0.3 0.4 ...... # 而是会出现 像 0.19999999998 0.289999999999 0.3xxxxxxxxxxx 这种情况 # 因此需要对其进行一个round操作，保留小数点后一位 n = round(n, 1)# 将地区名称及其对应的初始评分字典添加到ratedicAll中# &#123; &#123; 地区名称：&#123;评分：数量，评分：数量，评分：数量，......&#125; &#125; ，...... &#125;for c in countrylist: ratedicAll.append(&#123;c:ratedic.pop()&#125;)# 用于存放将要显示在图表中的副标题# 格式：地区 电影数量 地区 电影数量 地区 电影数量......fubiaoti = &apos;&apos;for r in ratedicAll: # r 就是 &#123; 地区名称：&#123;评分：数量，评分：数量，评分：数量，......&#125; &#125; i = 0 # 暂存某地区电影数量 fubiaoti += r.keys()[0] # 将每个地区名称添加到副标题中 # 处理文件中的评分数据 for line in f[r.keys()[0]].readlines(): try: rate = line.split(&apos;^&apos;)[1] # 拿到评分 r.values()[0][float(rate)] += 1 # 对应评分加1 r.values()[0]就是&#123;评分：数量，评分：数量，评分：数量，......&#125; i += 1 # 电影数量加1 except ValueError: continue print i fubiaoti += &apos; &apos;+str(i)+&apos; &apos; # 处理完后副标题加上对应的电影数量# 将所有文件关闭for c in countrylist: f[c].close()# 创建折线图line = Line(&quot;各国家/地区电影豆瓣评分数据&quot;,fubiaoti,width=1300,height=600)for r in ratedicAll: # 参数：线对应的名称、X轴(评分)、Y轴(按键值(评分)从小到大的顺序输出其值到对应的X坐标)(数量)、线外观属性的设置...... line.add(r.keys()[0], x, map(lambda x:x[1],sorted(r.values()[0].items(),key=lambda d:d[0])),\ is_fill=True, line_opacity=0.2, area_opacity=0.4, symbol=None,xaxis_interval=9, is_smooth=True,legend_top=&apos;bottom&apos;) # render出HTML文件line.render(r&apos;D:\allmovice.html&apos;) 执行该代码后，在D盘生成allmovice.html，打开如下 另外附上 pyecharts文档 更详细更高级的用法都可以查阅到。&emsp;&emsp; 注：以上基于python2.x、Windows平台]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫——豆瓣电影]]></title>
    <url>%2Fpython%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;近期博主片荒，没啥视频看，就想去豆瓣找几部高分电影解解眼馋。豆瓣上的Top250挺不错的，不过大多数我喜欢的类型都看过，如果能来个个Top500、Top1000就好了。想到这于是冒出了个念头，不如把豆瓣上现有的电影都爬取下来，顺便做个评分数据的可视化玩玩还挺不错的。心动不如行动，撸起袖子就是干。打开豆瓣电影选影视页面https://movie.douban.com/tag/#/ 并勾选上电影，从页面中的“加载更多”可以得到这个页面是动态加载的。因此我们直接打开开发者工具，然后点击网页中的“加载更多”，可以看到浏览器下载了很多jpg和一个文件点开查看可以发现这个正是新加载的电影的一些信息，包括电影名称、评分、导演、演员、电影链接等等。&emsp;&emsp;找到了这个数据API接口，那我们就可以开爬了。不过想到后面想用这些数据做个简单的评分数据可视化，于是再次观察下这个接口的请求链接。在链接里有个tags=%E7%94%B5%E5%BD%B1，其中%E7%94%B5%E5%BD%B1正是“电影”的URL编码。在网页中我们勾选上“大陆”，可以看到网页更新了，打开刚下载的电影信息文件可以看到链接跟之前的不太一样，多了%E5%A4%A7%E9%99%86，我们将其解码得到的是大陆二字。至此接口链接的解析工作基本完成，只要改变地区的编码，就能拿到相应地区的电影数据，最后的start表示起始位置，每次访问接口它会返回20条数据，因此start的值由0开始，间隔20。下面我们进入代码具体看下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201# -*- coding:utf-8 -*-## 爬取豆瓣所有电影条目，以地区为分类# 由于爬取所用的链接所连接的数据库不是最新的，因此# 所爬取的电影条目会比真实的数据条目少一些，保守估计# 每一个分类比真实的数据条目少50左右# 由于豆瓣电影的分类规则原因，不同地区的电影存在的重合import sysreload(sys)sys.setdefaultencoding(&apos;utf-8&apos;)import requestsimport reimport osimport jsonimport urllib2import randomfrom time import sleep,timeimport multiprocessingfrom multiprocessing import Pooldef makefile(name): &apos;&apos;&apos; 函数： makefile 参数： name（地区中文名称） 功能： 建立对应电影存储文件 返回： 文件f &apos;&apos;&apos; dir = os.path.split(os.path.realpath(__file__))[0] f = open(r&apos;%s\%s_movie.txt&apos; % (dir, name.encode(&apos;GB18030&apos;)), &apos;w&apos;) f.write(&apos;电影名称&apos;.encode(&apos;GB18030&apos;)) f.write(&apos;^&apos;) # 分隔符，方便后期提取数据 f.write(&apos;豆瓣评分&apos;.encode(&apos;GB18030&apos;)) f.write(&apos;^&apos;) f.write(&apos;导演s&apos;.encode(&apos;GB18030&apos;)) f.write(&apos;^&apos;) f.write(&apos;演员s&apos;.encode(&apos;GB18030&apos;)) f.write(&apos;^&apos;) f.write(&apos;豆瓣链接&apos;.encode(&apos;GB18030&apos;)) f.write(&apos;\n&apos;) return fclass Unbuffered(object): &apos;&apos;&apos; 该类用来设置系统控制台非缓冲输出， 方便观看每个进程的执行状况。 &apos;&apos;&apos; def __init__(self, stream): self.stream = stream def write(self, data): self.stream.write(data) self.stream.flush() def __getattr__(self, attr): return getattr(self.stream, attr) def getinfo(url, country, name, ip): &apos;&apos;&apos; 函数： getinfo 参数： country（地区对应的URL编码）name（地区中文名称）ip（ip代理池） 功能： 获取电影信息 &apos;&apos;&apos; # 设置非缓冲输出 sys.stdout = Unbuffered(sys.stdout) # 输出进程pid print &apos;children--&apos;,os.getpid() # 设置请求头 user_agent =&apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0&apos; headers = &#123;&apos;User-Agent&apos;:user_agent&#125; # 获得写入文件 f = makefile(name) # 页面start，以20为间隔 n = 0 # 由于免费代理的不稳定性，j用于计数打开页面失败次数 j = 0 # 爬虫开始时间 btime = time() proxy = &#123;&apos;http&apos;:&apos;0,0,0,0:0&apos;&#125; while True: # 代理设置 proxy[&apos;http&apos;] = random.choice(ip) # 从代理池中随机选取一个代理 proxy_handler = urllib2.ProxyHandler(proxy) opener = urllib2.build_opener(proxy_handler) urllib2.install_opener(opener) tempurl = url+str(n) # 拼接接口链接 req = urllib2.Request(url=tempurl,headers=headers) try: # 获取页面内容 html = urllib2.urlopen(req).read() # 调用json将内容格式化成字典 # 该数据的格式为 &#123; &quot;data&quot;:[ &#123; 电影数据 &#125; ......] &#125; dic = json.loads(html) # 判断内容是否为空，若为空则说明爬取完成。 if len(dic[&quot;data&quot;]) == 0: break # 内容非空则处理每一条数据 for movice in dic[&quot;data&quot;]: # 电影名称 f.write(movice[&quot;title&quot;].encode(&apos;GB18030&apos;)) f.write(&apos;^&apos;) # 分隔符 # 评分 f.write(movice[&quot;rate&quot;].encode(&apos;GB18030&apos;)) f.write(&apos;^&apos;) # 导演 for director in movice[&quot;directors&quot;]: f.write(director.encode(&apos;GB18030&apos;)) f.write(&apos; &amp; &apos;) if len(movice[&quot;directors&quot;]) &gt; 0: f.seek(-3,1) f.write(&apos;^&apos;) # 演员 for cast in movice[&quot;casts&quot;]: f.write(cast.encode(&apos;GB18030&apos;)) f.write(&apos; &amp; &apos;) if len(movice[&quot;casts&quot;]) &gt; 0: f.seek(-3,1) f.write(&apos;^&apos;) # 豆瓣链接 f.write(movice[&quot;url&quot;].encode(&apos;GB18030&apos;)) f.write(&apos;\n&apos;) n += 20 j = 0 except urllib2.URLError as e: print e.reason j += 1 # 检测失败次数，若达到5次则跳过该页 if j == 5: print &apos;drop&apos;,name.encode(&apos;GB18030&apos;),n n += 20 continue f.close() # 爬虫结束时间 etime = time() # 输出地区名称及该爬虫的运行时间 print name.encode(&apos;GB18030&apos;),&apos;--&apos;,etime-btime if __name__ == &apos;__main__&apos;: # 设置非缓冲输出 sys.stdout = Unbuffered(sys.stdout) # 地区 country = [&quot;大陆&quot;, &quot;美国&quot;, &quot;香港&quot;, &quot;台湾&quot;, &quot;日本&quot;, &quot;韩国&quot;, &quot;英国&quot;, &quot;法国&quot;, &quot;德国&quot;, \ &quot;意大利&quot;, &quot;西班牙&quot;, &quot;印度&quot;, &quot;泰国&quot;, &quot;俄罗斯&quot;, &quot;伊朗&quot;, &quot;加拿大&quot;, &quot;澳大利亚&quot;, &quot;爱尔兰&quot;, &quot;瑞典&quot;, &quot;巴西&quot;, &quot;丹麦&quot;] # 地区URL编码 cDecode = [ &apos;%E5%A4%A7%E9%99%86&apos;, &apos;%E7%BE%8E%E5%9B%BD&apos;, &apos;%E9%A6%99%E6%B8%AF&apos;, &apos;%E5%8F%B0%E6%B9%BE&apos;, &apos;%E6%97%A5%E6%9C%AC&apos;, &apos;%E9%9F%A9%E5%9B%BD&apos;, &apos;%E8%8B%B1%E5%9B%BD&apos;, &apos;%E6%B3%95%E5%9B%BD&apos;, &apos;%E5%BE%B7%E5%9B%BD&apos;, &apos;%E6%84%8F%E5%A4%A7%E5%88%A9&apos;, &apos;%E8%A5%BF%E7%8F%AD%E7%89%99&apos;, &apos;%E5%8D%B0%E5%BA%A6&apos;, &apos;%E6%B3%B0%E5%9B%BD&apos;, &apos;%E4%BF%84%E7%BD%97%E6%96%AF&apos;, &apos;%E4%BC%8A%E6%9C%97&apos;, &apos;%E5%8A%A0%E6%8B%BF%E5%A4%A7&apos;, &apos;%E6%BE%B3%E5%A4%A7%E5%88%A9%E4%BA%9A&apos;, &apos;%E7%88%B1%E5%B0%94%E5%85%B0&apos;, &apos;%E7%91%9E%E5%85%B8&apos;, &apos;%E5%B7%B4%E8%A5%BF&apos;, &apos;%E4%B8%B9%E9%BA%A6&apos; ] # 生成地区及其URL编码一一对应的字典 dicCountry = dict(zip(cDecode, country)) print &apos;main--&apos;,os.getpid() # 获取ip代理池 dir = os.path.split(os.path.realpath(__file__))[0] ipfile = open(r&apos;%s\ipPool.txt&apos; % (dir)) iplist = [] for line in ipfile.readlines(): iplist.append(line.strip(&apos;\n&apos;)) ipfile.close() begintime = time() # 多进程爬取 pool = Pool(processes=4) for c in dicCountry: # 拼接原始链接 turl = &apos;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=0,10&amp;tags=%E7%94%B5%E5%BD%B1,&apos;+c+&apos;&amp;start=&apos; # print turl pool.apply_async(getinfo, (turl, c, dicCountry[c], iplist)) # 调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了 pool.close() # 等待所有子进程执行完毕再继续执行 pool.join() endtime = time() print &apos;main--&apos;,endtime-begintime print &apos;done&apos; 下面是爬取西刺免费代理的脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# -*- coding:utf-8 -*-# # 建立/更新 IP 代理池# 来源网站：西刺 http://www.xicidaili.comimport sysreload(sys)sys.setdefaultencoding(&apos;utf-8&apos;)import urllib2import osimport refrom lxml import etree# 西刺高匿代理起始页面url = &apos;http://www.xicidaili.com/nn/&apos;# 代理ip有效性测试网站test_url = &quot;https://www.baidu.com/&quot;# urllib2请求头设置user_agent =&apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0&apos;headers = &#123;&apos;User-Agent&apos;:user_agent&#125;def checkipwork(proxy): &apos;&apos;&apos; 函数： checkipwork 参数： proxy (数据类型：字典 &#123;&apos;http&apos;:&apos;0.0.0.0:0&apos;&#125;) 功能： 测试代理ip有效性 返回： 有效：True 无效：False &apos;&apos;&apos; # 代理设置 proxy_handler = urllib2.ProxyHandler(proxy) opener = urllib2.build_opener(proxy_handler) # urllib2.install_opener(opener) req = urllib2.Request(url=test_url,headers=headers) try: # res = urllib2.urlopen(req) res = opener.open(req) content = res.readline() # 检查是否有返回内容及其状态码 if content and res.code == 200: return True else: print proxy[&apos;http&apos;],res.code,&apos;its not ok&apos; return False except urllib2.URLError as e: print e.reason,proxy[&apos;http&apos;],&apos;its not ok&apos; return False def getLatestIp(): &apos;&apos;&apos; 函数： getLatestIp 参数： 功能： 建立/更新 IP 代理池，将符合条件的ip存入当前文件夹 ipPool.txt 文件中 返回： &apos;&apos;&apos; pattern = re.compile(r&apos;&lt;tr class=.*?tr&gt;&apos;, re.S) pattern2 = re.compile(r&apos;&quot;bar_inner (.*?)&quot;&apos;, re.S) proxy = &#123;&apos;http&apos;:&apos;0.0.0.0:0&apos;&#125; # 获取当前文件夹路径 dir = os.path.split(os.path.realpath(__file__))[0] # 打开写入ip的文件 ipPool.txt f = open(r&apos;%s\ipPool.txt&apos; % (dir), &apos;w&apos;) # 处理过程的日志文件 log = open(r&apos;%s\getLatestIpLog.txt&apos; % (dir), &apos;w&apos;) log.write(ctime()+&apos;\nbegin......\n&apos;) # n：页面数 n = 1 # j：用于计数打开页面失败的次数 j = 0 # total：用于计数总的可用的ip数 total = 0 # 只抓取前10页的ip while n &lt; 21: try: tempurl = url + str(n) req = urllib2.Request(url=url,headers=headers) log.write(&apos;processing &apos;+tempurl+&apos;\n&apos;) html = urllib2.urlopen(req) page = html.read() # 获取网页中每个代理的DOM list = re.findall(pattern, page) t = 0 for l in list: # 获取代理的速度标签 # 本函数过滤掉所有连接时间跟连接速度小于fast的 r = re.findall(pattern2, l) if r[0] == &apos;fast&apos; and r[1] == &apos;fast&apos;: # 使用xpath快速获取ip及对应port temp = etree.HTML(l) ip = temp.xpath(&apos;//tr/td[2]/text()&apos;)[0] port = temp.xpath(&apos;//tr/td[3]/text()&apos;)[0] # 设置临时代理字典并传入checkipwork进行有效性检测 proxy[&apos;http&apos;] = ip+&apos;:&apos;+port if checkipwork(proxy): # 若检测有效则写入文件 f.write(ip+&apos;:&apos;+port+&apos;\n&apos;) total += 1 t += 1 print n log.write(&apos;Done: this page get %d ip, truly collect %d.\n&apos; % (len(list), t)) j = 0 n += 1 sleep(2) except urllib2.URLError as e: log.write(&apos;failed......\n&apos;) print e.reason,&apos;main&apos; j += 1 log.write(&apos;failed %d times\n&apos; % (j)) # j：用于计数打开页面失败的次数 # 若失败次数达到5次，则跳过该页 if j == 5: n += 1 j = 0 log.write(&apos;skip this page\n&apos;) sleep(5) continue print total log.write(&apos;end......\ntotally Processing %d pages\ntotally collecting %d ip\n&apos; % (n-1, total)) log.close() f.close() if __name__ == &apos;__main__&apos;: getLatestIp() 爬取豆瓣电影之前最好运行该脚本更新ipPool.txt文件内的ip，以保证ip有较高的可用性。 爬取完成后会在当前目录下得到如下文件至于剩下来的评分数据可视化，我们在另外一篇文章：pyecharts——连接Echarts与Python里做一个简单的应用跟展示。&emsp;&emsp; 注：以上基于python2.x、Windows平台 &emsp;&emsp;参考： 廖雪峰的python教程之多线程 Python多进程编程]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫——PyV8]]></title>
    <url>%2Fpython%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94PyV8.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;上一篇爬虫文章我们通过直接找到数据API接口从而绕开了网页动态加载的问题。那么如果在动态网页中找不到数据接口那肿么破呢？！接下来我们来看看python爬虫中应对动态网页的另一种姿势：PyV8执行JS脚本。&emsp;&emsp;上次我们下载了所有王者荣耀英雄的皮肤图片，这次我们来爬取LOL所有英雄的皮肤图片。打开某个英雄的页面http://lol.qq.com/web201310/info-defail.shtml?id=Amumu（阿木木），右键查看源代码可以发现这个网页明显是动态加载的 F12打开开发者工具再刷新页面从中可以看到我们所要爬取的图片，从链接来看跟某个编号有关。再看看其他可以看到一个很显眼js条目打开其链接得到如下从中可以看到id key name title skins 等等这些关键信息，如取其中title输出来看很明显这个js文件存放的是该英雄的所有信息（你可以再输出其他Unicode字符串看看是哪些具体信息）其中skins中的id和上面看到的图的名称bigxxxx.jpg都一一对应，到这我们可以想到，只要拿到所有英雄的英文名称，就能拿到所有英雄的js文件，那么就能拿到skins中的所有编号，这样就能把所有皮肤图片爬取下来。 那现在怎么拿到所有英雄的英文名称呢在网络栏的众多条目中，我们找到另外一个有价值的js条目：champion.js打开其链接得到如下很明显这些正是英雄的英文名称及其对应的编号。 &emsp;&emsp;到这我们爬虫所需要的东西都到齐了，有的同学可能会说，直接用正则把需要的信息匹配出来不就行了，都不用执行什么JS脚本。确实直接用正则将需要的信息匹配出来可以爬取所有图片，但是呢会很繁琐。不但要处理英文名称，中文名称，描述，编号，id等等，还要考虑不同英雄的皮肤数量不同，皮肤编号及其对应名称这些。这样处理起来会非常的不方便。&emsp;&emsp;这时我们转变一下思路，既然它们都是js文件，信息又是以js中的数组形式表示着，那我们能不能将这些数据结构化地拿到python中呢？！ 这时候就需要用到PyV8模块了&emsp;&emsp;PyV8是一个Python封装V8引擎的壳。它提供了简单可用的API，能够利用python来构建出JavaScript的运行时环境。下面我们通过代码来看看PyV8的简单应用。爬虫脚本如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding:utf-8 -*-import sys reload(sys) sys.setdefaultencoding(&apos;utf-8&apos;)import urllib2import requestsimport PyV8import osfrom time import sleepimport random# 存有全英雄信息的js链接url = &quot;http://lol.qq.com/biz/hero/champion.js&quot;# 获取该js脚本html = urllib2.urlopen(url)# 创建一个jsContext对象并进入ctxt = PyV8.JSContext()ctxt.enter()# 执行js代码ctxt.eval(html.read())# 用JSContext对象的locals方法获取js中的变量# 该结果是英雄的id及其对应的英文名字构成的类字典对象# 注意：它不是字典 而是一个&lt;class &apos;_PyV8.JSObject&apos;&gt;，它有两个方法：keys 和 clone，并且不支持迭代# 因此下面for循环不能直接用它进行迭代all_hero = ctxt.locals.LOLherojs.champion[&quot;keys&quot;]# 用该对象的keys方法拿到所有键，返回的是列表# 注意：在浏览器中我们看到的键是字符串，但是这里返回来后是int型hero_id = all_hero.keys()for id in hero_id: hero_name = all_hero[str(id)] # 用id键去all_hero取对应的值时又得把它转成str url = &quot;http://lol.qq.com/biz/hero/&quot;+hero_name+&quot;.js&quot; # 拼接存有详细英雄信息的js链接 html = urllib2.urlopen(url) ctxt.eval(html.read()) # 以下是另一种从js中获取变量值的方式（执行一个js函数进行return） func = ctxt.eval(&apos;&apos;&apos; (function()&#123; return LOLherojs.champion[&quot;%s&quot;].data.name+&quot; &quot;+LOLherojs.champion[&quot;%s&quot;].data.title; &#125;) &apos;&apos;&apos; % (hero_name,hero_name)) hero_ZHname = func() # 得到函数返回结果（英雄中文名称） os.mkdir(&quot;D:\\LOLwallpaper2\\&quot;+hero_ZHname.encode(&apos;GB18030&apos;)) os.chdir(&quot;D:\\LOLwallpaper2\\&quot;+hero_ZHname.encode(&apos;GB18030&apos;)) skins = ctxt.locals.LOLherojs.champion[hero_name].data.skins # 获取皮肤编号列表 for skin in skins: skin_links=&apos;http://ossweb-img.qq.com/images/lol/web201310/skin/big&apos;+skin.id+&apos;.jpg&apos; img = requests.get(skin_links) if img.status_code == 200: open(skin.name.encode(&apos;GB18030&apos;)+&apos;.jpg&apos;, &apos;wb&apos;).write(img.content) sleep(random.randint(1,3)) # 防ban print hero_name,&apos; done&apos; 为更好理解以上脚本最好对JavaScript有些许了解 PyV8安装参考 PyV8文档]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫——F12...寻找数据API接口]]></title>
    <url>%2Fpython%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94F12-%E5%AF%BB%E6%89%BE%E6%95%B0%E6%8D%AEAPI%E6%8E%A5%E5%8F%A3.html</url>
    <content type="text"><![CDATA[基于python2.x、Windows平台 &emsp;&emsp;在上一篇python爬虫——简单的网页提取中我们了解了基本的网页提取，但有时候我们要爬取的内容并不能从网页中获取得到。比如说，还是王者荣耀，每个英雄的皮肤图片都很好看，把它们下载下来作桌面壁纸也不错。我们打开某个英雄的页面http://pvp.qq.com/web201605/herodetail/106.shtml，查看源代码，搜索skin得到仅有3处 &emsp;&emsp;从链接来看我们基本能猜到皮肤图片的网页链接的结构，尝试一下就能确定了。比如在浏览器中直接打开 http://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/106/106-bigskin-1.jpg ，我们可以得到一张高清英雄皮肤图片。那么我们的思路基本确定下来了：得到英雄的名字及其对应编号（如小乔链接中的106）和皮肤数量，然后依次拼接这些图片链接并打开将它们下载下来。&emsp;&emsp;那么如何得到英雄的名字及其对应编号（如小乔链接中的106）和皮肤数量？！最简单的方法就是自己手动一个一个网页地看，这显然不是我们要的。那要怎么做呢？！&emsp;&emsp;在王者荣耀官网有这样一个网页http://pvp.qq.com/web201605/herolist.shtml，在里面可以看到所有的英雄。然而查看它的源代码却发现并没有这些英雄的信息。那么可以确定这是一个动态网页，网页内容由js动态加载的。对于这类网页要如何处理呢。&emsp;&emsp;其中一种方法是使用selenium模块，它是一个自动化测试工具。它支持各种浏览器，包括 Chrome，Safari，Firefox 等主流界面式浏览器。也有无界面浏览器PhantomJS（一个无界面的,可脚本编程的WebKit浏览器引擎）。它原生支持多种web 标准：DOM 操作，CSS选择器，JSON，Canvas 以及SVG。使用selenium模块处理动态网页就是用”浏览器”打开网页，等网页的内容加载完成后再进行信息提取。跟我们手动刷网页差不多，只不过它是自动的。使用这种方法能达到我们的目的，但是效率比较低。&emsp;&emsp;另外一种方法是在网页中寻找我们要爬的数据的API接口。内容能显示在网页上，说明浏览器已经将它下载下来。那它是什么时候下载，从哪里下载，这个就是我们要找的。如何找——F12（浏览器的开发者工具）&gt;本篇以火狐浏览器为例打开刚才的页面，等页面加载完后打开开发者工具，在查看器项目可以看到加载完后网页的代码。切换到网络标签，F5刷新页面，从中我们可以看到浏览器打开该链接后从服务器那里都下载了什么东西下来。在这些众多的条目中我们发现有一个条目herolist.json，点开这个条目查看其响应可以看到，这个json包含的正是我们所要找的信息，包括英雄编号，名字，描述，皮肤名称等等，在消息头标签可以看到其对应的网址。这样对于所要爬取的信息，我们就不用等待网页加载完再提取，而是直接打开这个网址就能得到我们所要的信息。 so 我们的爬虫脚本如下：123456789101112131415161718192021222324252627282930313233343536# -*- coding:utf-8 -*-import sys reload(sys) sys.setdefaultencoding(&apos;utf-8&apos;)import requestsimport osdef downloadPicture(): &apos;&apos;&apos; 下载并保存图片 &apos;&apos;&apos; # 英雄信息json的链接 url=&apos;http://pvp.qq.com/web201605/js/herolist.json&apos; # 设置请求头 head=&#123;&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101&apos;&#125; html = requests.get(url,headers=head) # 获取json数据，其结果是一个列表 html_json=html.json() for hero in html_json: hero_name = hero[&apos;cname&apos;] # 英雄名字 hero_title = hero[&apos;title&apos;] # 英雄描述 hero_id = hero[&apos;ename&apos;] # 英雄编号 os.mkdir(&quot;D:\\WZRYwallpaper2\\&quot;+hero_name+&apos; &apos;+hero_title) # 创建英雄目录 os.chdir(&quot;D:\\WZRYwallpaper2\\&quot;+hero_name+&apos; &apos;+hero_title) # 进入刚创建的目录 skin_names = hero[&apos;skin_name&apos;].split(&apos;|&apos;) # 得到英雄皮肤名称列表 skin_index = 1 for skin_name in skin_names: skin_links=&apos;http://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/&apos;+str(hero_id)+&apos;/&apos;+str(hero_id)+&apos;-bigskin-&apos;+str(skin_index)+&apos;.jpg&apos; img = requests.get(skin_links) if img.status_code == 200: open(skin_name+&apos;.jpg&apos;, &apos;wb&apos;).write(img.content) # 以皮肤对应的名称保存jpg skin_index += 1if __name__==&apos;__main__&apos;: downloadPicture() 运行下脚本，不用一会就可以去收图啦~~ 关于requests模块：&emsp;&emsp;requests中文文档 tip：网页爬虫多按F12]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫——简单的网页提取]]></title>
    <url>%2Fpython%E7%88%AC%E8%99%AB%E2%80%94%E2%80%94%E7%AE%80%E5%8D%95%E7%9A%84%E7%BD%91%E9%A1%B5%E6%8F%90%E5%8F%96.html</url>
    <content type="text"><![CDATA[基于python2.x、Windows平台 &emsp;&emsp;我们的目标是使用urllib2模块获取王者荣耀英雄的“英雄故事”和“历史上的ta”的内容。比如这个页面：http://pvp.qq.com/web201605/herodetail/106.shtml（小乔）&emsp;&emsp;打开这个页面并右键选择查看网页源代码，ctrl+f搜索英雄故事可以看到如下内容：那两段话就是我们要提取的内容。 爬虫脚本如下：1234567891011121314# -*- coding:utf-8 -*-import reimport urllib2url=&apos;http://pvp.qq.com/web201605/herodetail/106.shtml&apos;html = urllib2.urlopen(url) #打开网页text = html.read() #获取网页源代码pattern = re.compile(&apos;&lt;h2 class=&quot;cover-name&quot;&gt;(.*?)&lt;/h2&gt;&apos;) #匹配对应英雄名字result = re.search(pattern,text)print result.group(1) #输出英雄名字pattern = re.compile(&apos;&lt;div class=&quot;pop-bd&quot;&gt;.*?&lt;p&gt;(.*?)&lt;/p&gt;.*?&lt;/div&gt;&apos;, re.S) #匹配要抓取的两段文字result = re.findall(pattern, text)print u&apos;英雄故事：&apos;.encode(&apos;GB18030&apos;),result[0]print u&apos;历史上的ta：&apos;.encode(&apos;GB18030&apos;),result[1] &emsp;&emsp;输出结果： 更多的进阶用法：&emsp;&emsp;深入解析Python中的urllib2模块]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3编码小记]]></title>
    <url>%2FPython3%E7%BC%96%E7%A0%81%E5%B0%8F%E8%AE%B0.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;相比于Python 2 的编码，开发人员在Python 3 的编码上改进了许多。本篇将对Python 3 的编码作一个小记，并列出一些需要注意的地方。 &emsp;&emsp;首先来看一下Python 3 的编码情况，分别在Windows和Linux下运行以下脚本：1234567import sysimport localeprint (sys.getdefaultencoding()) #系统默认编码print (sys.getfilesystemencoding()) #文件系统编码print (locale.getdefaultlocale()) #系统当前编码print (sys.stdin.encoding) #终端输入编码print (sys.stdout.encoding) #终端输出编码 Windows下输出： 12345utf-8utf-8(&apos;zh_CN&apos;, &apos;cp936&apos;)cp936cp936 Linux下输出： 12345utf-8utf-8(&apos;zh_CN&apos;, &apos;utf-8&apos;)utf-8utf-8 &emsp;&emsp;可以看到，Python 3 的系统默认编码不在是ASCII，而是utf-8，我们在编写带有中文的源码文件时不再需要在开头加上# -*- coding：utf-8 -*-。 接下来看看Python 3 的字符串。&emsp;&emsp;Python 3 中字符串分两种类型：bytes 和 str。 bytes：某种编码（UTF-8，GBK等）类型的字节序列，普通字符串加上字母b作为前缀，就是表示bytes字符串了。它在内存中就是一串01。如果我们打印一个bytes字符串，我们可以看到它是以某种编码的形式呈现的，这是因为如果输出一串01，对于人来说读不懂，因此Python 3 在输出打印bytes字符串时做了调整，将其以自身编码的方式呈现出来。 str：Unicode类型的字符串（Unicode编码），没有加b前缀的字符串。 Python 3 的编码优化让我们不再经常碰到编码出错的问题，但还是有需要注意的地方。 在Windows下在Windows下在Windows下（以下均在Windows下测试） &emsp;&emsp;如果我们用open(&#39;xxxx&#39;)的形式打开一个文件，并且read文件的内容到一个str，那么该字符串的编码是文件的编码，而不是Unicode。比如说有一个txt文件是以utf-8编码的，我们用open(&#39;xxxx&#39;)的形式打开并将内容输出到cmd（gbk编码），那么将会出现乱码。或者将其写到另外一个文件，那么该文件也将是utf-8编码。 比如，新建一个txt文件，写上编码测试，并用notepad将其编码转换成utf-8无bom编码。执行下面的脚本： 12345f = open(&apos;xxxx.txt&apos;)s = f.read()print (type(s))print (s)f.close() 输出： 12&lt;class &apos;str&apos;&gt;缂栫爜娴嬭瘯 &emsp;&emsp;如果我们在打开文件的时候告诉Python该文件的编码，那么它就能将读取到的内容顺利转换成Unicode编码，我们再进行输出操作就不会有乱码问题了。代码修改如下： 12345f = open(&apos;xxxx.txt&apos;, encoding=&apos;utf-8&apos;)s = f.read()print (type(s))print (s)f.close() 输出 12&lt;class &apos;str&apos;&gt;编码测试 &emsp;&emsp;还有一种方法，这里博主不推荐，不过还是说一下，以备不时之需。读取文件内容之后，用要输出到的目标平台的编码且该编码支持文件内容的字符的编码去把读取到内容的str encode成bytes，再用文件内容的原始编码去decode刚刚的bytes。 12345f = open(&apos;xxxx.txt&apos;)s = f.read().encode(&apos;gbk&apos;).decode(&apos;utf-8&apos;)print (type(s))print (s)f.close() 输出： 12&lt;class &apos;str&apos;&gt;编码测试 以上是读文件需要注意的地方，下面看看写文件方面。 如果我们用open(&#39;xxxx&#39;, &#39;w&#39;)的形式打开一个文件，向其写入内容，将会有如下的情况： 写入的内容不是Unicode编码，且含有非ASCII字符（比如中文），那么文件的编码跟写入内容的原始编码一样。 写入的内容是Unicode编码，且含有非ASCII字符（比如中文），那么文件是gbk编码。 如果我们用open(&#39;xxxx&#39;, &#39;w&#39;, encoding=&#39;xxx&#39;)的形式打开一个文件，向其写入内容，将会有如下的情况： 写入的内容不是Unicode编码，encoding将不起作用，文件的编码跟写入内容的原始编码一样。 写入的内容是Unicode编码，且含有非ASCII字符（比如中文），那么文件是xxx编码 以上均在Windows上测试得到的，各位可以验证看看。 然而在Linux下，Python 3 跟Windows又有不一样的地方。&emsp;&emsp;当用open(&#39;xxxx&#39;)的形式打开一个gbk编码文件，并read()里面内容的时候报错了：UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xb1 in position 0: invalid start byte&emsp;&emsp;而在Windows下用open(&#39;xxxx&#39;)的形式打开一个utf-8编码文件，并read()里面内容的时候并不会报错。 个人认为原因应该是：在Linux下Python 3 对于读取进来的内容，都会将其转换成Unicode编码，而不会保留其原来的编码，而在Windows下则会。 解决该问题的方法就是打开文件的时候指明其编码open(&#39;xxxx&#39;, encoding=&#39;xxx&#39;)。 编码建议：&emsp;&emsp;open文件时指明文件的编码，保持str全都是Unicode编码。 以上若有错误之处还望各位看官指正，非常感谢。 参考https://www.crifan.com/summary_python_string_encoding_decoding_difference_and_comparation_python_2_x_str_unicode_vs_python_3_x_bytes_str/]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python2编码解析]]></title>
    <url>%2FPython2%E7%BC%96%E7%A0%81%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;Python 2 的编码问题一直是许多pythoner诟病的问题，给人以剪不断理还乱的感觉。本篇将介绍python2.x对字符编码是如何处理的，以及编程中遇到的一些编码问题，并给出解决方案。&emsp;&emsp;（本文存在一些博主个人见解的概念，非官方概念，请各位看官自行理解）。 &emsp;&emsp;python编程中会经常遇到操作系统编码、文件编码、控制台输入输出编码、网页编码、源代码编码、python编码，本文将会逐一介绍。首先我们来看看一些常见的编码情况：12345print sys.getdefaultencoding() #系统默认编码print sys.getfilesystemencoding() #文件系统编码print locale.getdefaultlocale() #系统当前编码print sys.stdin.encoding #终端输入编码print sys.stdout.encoding #终端输出编码 将以上这段代码在windows与linux系统下分别运行，查看输出结果。 windows终端结果:12345asciimbcs(&apos;zh_CN&apos;, &apos;cp936&apos;)cp936cp936 Linux终端结果： 12345asciiUTF-8(&apos;zh_CN&apos;, &apos;UTF-8&apos;)UTF-8UTF-8 &emsp;&emsp;为什么会出现编码问题呢，很明显代码中出现编码不匹配的情况。如果你是纯英文编程，也不用处理到中文字符的，我想你大概也不会碰到编码的问题。但是在大天朝，怎么可能不涉及到中文呢。。。 最常遇到的错误就是：UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe4 in position x: ordinal not in range(xxx)UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode characters in position x-x: ordinal not in range(xxx) 首先来讲一下大家都知道的 # -*- coding:utf-8 -*-&emsp;&emsp;python 2 对于源代码的读取采用的是ASCII编码，所以你的源代码中有中文，在文件开头没写# -*- coding:utf-8 -*- ，那么你的文件运行就会报错：SyntaxError: Non-ASCII character xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # -*- coding:utf-8 -*- 的作用就是告诉python读取源代码用UTF-8编码，而非ASCII编码。 注意：# -*- coding:utf-8 -*-的作用仅仅是作用于源码读取而已。 所以你的源码中带有中文，请在开头加上# -*- coding:utf-8 -*-。 接下来我们来看看sys.getdefaultencoding() #系统默认编码。&emsp;&emsp;在了解sys.getdefaultencoding() #系统默认编码之前，我们非常有必要了解在Python 2 中字符串是如何被处理的。 Python 2 中字符串分两类 str 和 Unicode。 str：字符串前面 没有 加u的都是str类型，str有什么特点呢，如下： 若没有对str执行额外的encode或decode，那么python解释器不会对它进行额外的编码解码处理。&emsp;&emsp;比如该字符串是写在源码中，那么它的编码就是Python读取源码时用的编码。比如一个中文字符串s = &#39;中文&#39;，文件开头有# -*- coding:utf-8 -*-，那么该中文字符串就是UTF-8编码。如果直接将该字符串写到文件中，其编码还是UTF-8编码，因为该过程没有对其进行任何encode或decode。&emsp;&emsp;又如该字符串是从文件中读取的，比如在Windows中读取一个带有中文的文件，那么读取到的字符串一般就是GBK编码。若没有对它进行encode或decode操作，你可以直接将它输出而不会出现乱码。 所以对于str类型，不对它执行额外的encode或decode操作，Python会一直保持它原始的编码。 Unicode：字符串前面 有 加u的都是Unicode类型。&emsp;&emsp;脱开Python来说，Unicode是一个能包含全世界所有字符符号的字符集，它既是一个字符集，也是一种编码方式。&emsp;&emsp;在Python 2 中，Unicode是一个字符集，但“不作为一种输出编码方式”。 如何理解“不作为一种输出编码方式”。&emsp;&emsp;我们处理字符串，最终都是要输出出来，包括显示，存储到文件等等。Python 2 对于Unicode类型，如果要将其输出，必须以某种编码encode再输出。这其中包括显示的跟隐式的。&emsp;&emsp;比如说在Windows的控制台下输出s = u&#39;中国&#39;。我们知道Windows下cmd的默认编码是GBK，如果你在cmd的下执行脚本print s，则报错UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode characters in position 0-1: ordinal not in range(128)因为Python 2 中Unicode不作为输出编码方式，当你print时，Python隐式地对s执行encode操作，而encode成哪一种编码呢，那就是sys.getdefaultencoding() #系统默认编码这个系统默认编码了。Python 2 的默认系统编码是ASCII，而字符串中有中文，ASCII无法对中文编码，因此报错。 如何解决： 在进行print时显示地进行encode，如：print s.encode(&#39;GBK&#39;)。 或者修改系统默认编码： 1234import sysreload(sys)sys.setdefaultencoding(&apos;gbk&apos;)# Linux下为utf-8 总结下str和Unicode：&emsp;&emsp;Python中的Unicode我们可以把它理解为是一种底层处理编码，它是连接其他编码的桥梁，因为它能囊括全世界所有字符符号。比如你想把一个GBK编码的字符串转换成UTF-8编码，你可以先将其decode成Unicode，再从Unicode去encode成UTF-8，反之亦然。而str是一种除了Unicode编码之外的字符串类型，它所带的编码是可以用于输出操作，你可以对它进行decode操作成Unicode，也可以直接对它encode操作，python会隐式地在encode之前用系统编码对其进行decode成Unicode。 所以sys.getdefaultencoding() #系统默认编码的作用就是，当Python处理Unicode字符串的输出时，若发现没有显示地编码encode操作，则对该字符串以系统默认编码执行encode后输出。 提示：decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(&#39;gb2312&#39;)，表示将gb2312编码的字符串str1转换成unicode编码。encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(&#39;gb2312&#39;)，表示将unicode编码的字符串str2转换成gb2312编码。 讲了这么多，我们来实际操作看看吧。以下操作均在Windows下进行。 新建一个test.txt，在里面输入中文，比如：编码测试，直接保存退出。运行以下脚本： 12345f = open(&apos;test.txt&apos;)s = f.read()print sprint type(s)f.close() 输出： 12编码测试&lt;type &apos;str&apos;&gt; 可以看到，这个过程没有涉及任何编码解码操作，正常输出TXT里的内容。这是因为读进来的文件内容是GBK编码，cmd也是GBK编码，因此能够正常显示。 打开test.txt文件，将其另存为以UTF-8编码的形式。 注意：不要用记事本转成UTF-8，用notepad打开并在编码那里转成以UTF-8无BOM编码。记事本转成UTF-8带BOM会出错。这是UTF-8中带不带BOM的问题，这里先不说。 再次运行上面的脚本输出： 12锘跨紪鐮佹祴璇?&lt;type &apos;str&apos;&gt; 可以看到，编码测试变成锘跨紪鐮佹祴璇?。这是因为读进来的文件内容是UTF-8编码，与cmd的编码不一样，所以不能正常显示。 如何解决： 将读取进来的内容decode成Unicode再encode成GBK 12345f = open(&apos;test.txt&apos;)s = f.read()print s.decode(&apos;utf-8&apos;).encode(&apos;GBK&apos;)print type(s)f.close() 运行，输出： 12编码测试&lt;type &apos;str&apos;&gt; 正常显示。 再看下面这个脚本： 1234# -*- coding:utf-8 -*-a=u&quot;编码测试&quot; #定义一个Unicode字符串print type(a)print a 输出： 12&lt;type &apos;unicode&apos;&gt;UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 0-3: ordinal not in range(128) a字符串是Unicode字符串，对它执行print输出时，Python隐式地对它执行encode操作，encode编码默认是ASCII编码，不能编码中文，因此出现以上的错误。 如何解决： 修改系统编码 123import sysreload(sys)sys.setdefaultencoding(&apos;gbk&apos;) 在前面加上以上三条语句，将系统编码设置成gbk（Linux下设置成utf-8）脚本如下： 1234567import sysreload(sys)sys.setdefaultencoding(&apos;gbk&apos;)a=u&quot;编码测试&quot; #定义一个变量，为Unicode编码print type(a)print a 输出： 12&lt;type &apos;unicode&apos;&gt;编码测试 另外一种方法就是进行显示编码： 123a=u&quot;编码测试&quot; #定义一个变量，为Unicode编码print type(a)print a.encode(&apos;gbk&apos;) 输出： 12&lt;type &apos;unicode&apos;&gt;编码测试 正常输出。 再看非Unicode的情形： 1234# -*- coding:utf-8 -*-a=&quot;编码测试&quot; #定义一个变量，默认为Str，utf-8编码print aprint type(a) 输出： 12缂栫爜娴嬭瘯&lt;type &apos;str&apos;&gt; 因为# -*- coding:utf-8 -*-，所以a以utf-8编码，直接输出到控制台，由于编码不匹配所以出现上面的内容。 如何解决： 显示地进行解码编码： 1234# -*- coding:utf-8 -*-a=&quot;编码测试&quot; #定义一个变量，默认为Str，utf-8编码print a.decode(&apos;utf-8&apos;).encode(&apos;gbk&apos;)print type(a) 输出： 12编码测试&lt;type &apos;str&apos;&gt; 或者把系统编码设置成utf-8，那么可以直接对a进行encode操作 12345678# -*- coding:utf-8 -*-import sysreload(sys)sys.setdefaultencoding(&apos;utf-8&apos;)a=&quot;编码测试&quot; #定义一个变量，默认为Str，utf-8编码print a.encode(&apos;gbk&apos;) # 这个过程Python隐式地先执行decode(&apos;utf-8&apos;)操作print type(a) 提示：直接对str进行encode的话，Python会先对其进行隐式的decode操作，所用的编码是系统编码。 输出： 12编码测试&lt;type &apos;str&apos;&gt; &emsp;&emsp;所以对于Python 2 中的编码，我们只要抓住几个点就完全可以应对各种编码情况。 弄清楚字符串从哪里来，编码是什么，经过什么处理，要输出到什么平台，文件，其编码要求是什么。 Unicode是Python 2 “底层”的编码，用于处理的编码，协助str进行编码转换。输出Unicode必须经过encode操作，可显示的，也可隐式的。隐式的话需要注意此时的系统编码。 str是Python 2 “上层”的编码，用于处理、输出的编码，通过Unicode的协助完成编码转换。 str decode成Unicode Unicode encode成str 注意Python 2 中 显示的 及 隐式的 decode、encode操作。 最后 python 2 编码建议： 请尽量在Linux系统上编程，linux下较windows，编码问题良好很多。 python代码内部请全部使用unicode编码，在获取外部内容时，先decode为unicode，向外输出时再encode为Str 在定义变量或者正则时，也定义unicode字符，如a = u”中文”；res=r””+u”正则”。 以上若有错误之处还望各位看官指正，非常感谢。 参考：https://thief.one/2017/02/16/%E8%A7%A3%E5%86%B3Python2-x%E7%BC%96%E7%A0%81%E4%B9%8B%E6%AE%87/]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python正则表达式——re模块部分解析]]></title>
    <url>%2FPython%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E2%80%94%E2%80%94re%E6%A8%A1%E5%9D%97%E9%83%A8%E5%88%86%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;正则表达式，又称规则表达式。（英语：Regular Expression，在代码中常简写为regex、regexp或RE），计算机科学的一个概念。它是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。因此它通常被用来检索、替换那些符合某个模式(规则)的文本。&emsp;&emsp;Python的正则表达式处理由re模块提供，本篇将讲解re模块中经常被用到的几个函数的基本用法。&emsp;&emsp;以下操作均在Python 3 GUI 中进行。 引入Python的正则表达式处理由re模块提供，我们先来看一个例子： 123456s = &apos;Python正则表达式&apos;pattern = re.compile(&apos;正则表达式&apos;)result = re.search(pattern, s)print (type(result))print (result)print (result.gruop(0)) 输出： 123&lt;class &apos;_sre.SRE_Match&apos;&gt;&lt;_sre.SRE_Match object; span=(6, 11), match=&apos;正则表达式&apos;&gt;&apos;正则表达式&apos; &emsp;&emsp;以上演示了在一个字符串中寻找某个子串。其中re.compile的作用编译正则表达式模式为一个正则表达式对象，而该正则表达式模式为括号中的&#39;正则表达式&#39;。 re.search&emsp;&emsp;re.search是在字符串s中找到第一个与正则表达式对象pattern匹配的串后停止并返回结果，这意味着如果主串中有两个匹配的子串，re.search找到并返回的总是第一个出现的。&emsp;&emsp;从上面可以看到，re.search返回的结果类型是_sre.SRE_Match，（若匹配不到，结果则是None），直接打印该结果会得到两个比较有用的信息：匹配到的串及该串在主串中的位置。如果仅仅想要匹配的串，则需要用到_sre.SRE_Match类下的group。至于group我们稍后会将解。 注意，上面例子还有另外两种写法： 123s = &apos;Python正则表达式&apos;pattern = re.compile(&apos;正则表达式&apos;)result = pattern.search(s) 以及 12s = &apos;Python正则表达式&apos;result = re.search(&apos;正则表达式&apos;, s) 此处仅作一个补充，接下来所有的例子均采用最上面第一个例子的写法。 &emsp;&emsp;上面的例子讲到re.search返回的结果，该结果有一个group的方法，这个到底怎么用呢，看下面的例子： 1234567s = &apos;Python正则表达式&apos;pattern = re.compile(&apos;((正则)(表达))式&apos;) #这里并不是匹配括号，而是生成group的作用，若要匹配括号，要在括号前面加上反斜杠\，或在字符串前面加上r。ss = re.search(pattern, s)print (ss.group(0))print (ss.group(1))print (ss.group(2))print (ss.group(3)) 输出： 1234正则表达式正则表达正则表达 &emsp;&emsp;可以看到，如果re.search成功匹配，其结果有一个默认的group，就是0，它的结果是整个匹配串正则表达式。至于有没有group(1) group(2)…，就看你的模式有没有用括号进行group划分。 &emsp;&emsp;group的匹配顺序是由外到里，由左到右。先是外层括号，后是内层括号，同级括号的顺序从左到右。 在上面的例子中，group(1)对应的是正则表达，group(2)对应的是正则，group(3)对应的是表达。 re.matchre.search基本上就这样，再来看看与它相似的re.match。看下面的例子： 1234567s = &apos;Python正则表达式&apos;pattern = re.compile(&apos;Py(thon)&apos;)result = re.match(pattern, s)print (type(result))print (result)print (result.gruop(0))print (result.gruop(1)) 输出： 1234&lt;class &apos;_sre.SRE_Match&apos;&gt;&lt;_sre.SRE_Match object; span=(0, 6), match=&apos;Python&apos;&gt;&apos;Python&apos;&apos;thon&apos; 可以看到，和re.search返回的结果类型是一样的。 不同之处在于： 1234s = &apos;Python正则表达式&apos;pattern = re.compile(&apos;正则表达式&apos;)result = re.match(pattern, s)print (result) 输出： 1None 也就是说，re.match是从字符串的开头就开始匹配的，它不会像re.search一样在串里寻找匹配项。 re.findall最后看一下也经常用到的re.findall。 12345s = &apos;Python正则表达式，第二个Python正则表达式&apos;pattern = re.compile(&apos;正则表达式&apos;)result = re.findall(pattern, s)print (type(result))print (ss) 输出： 12&lt;class &apos;list&apos;&gt;[&apos;正则表达式&apos;, &apos;正则表达式&apos;] 可以看到，re.findall返回的结果是list，里面包含所有匹配的子串。 再进一步，看下面的例子： 12345s = &apos;Python正则表达式，第二个Python正则表达式&apos;pattern = re.compile(&apos;(正则)表达式&apos;) # 注意此处模式里加入了groupresult = re.findall(pattern, s)print (type(result))print (ss) 输出： 12&lt;class &apos;list&apos;&gt;[&apos;正则&apos;, &apos;正则&apos;] 对于re.findall，如果匹配模式里有group，则返回的结果不再是group(0)，而是该模式下的group(1) group(2) group(3)…… 结束&emsp;&emsp;以上介绍了re模块中经常被用到的4个函数的基本用法，掌握了这4个函数的基本用法基本上就可以处理Python中绝大多数有关于用正则表达式处理字符串的问题了。 以上若有错误之处还望各位看官指正，非常感谢。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>正则表达式</tag>
        <tag>字符串处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python正则表达式——书写正则表达式]]></title>
    <url>%2FPython%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E2%80%94%E2%80%94%E4%B9%A6%E5%86%99%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html</url>
    <content type="text"><![CDATA[本篇摘抄于网络，非作任何商业用途，若存在版权问题，请联系博主。 什么是正则表达式&emsp;&emsp;正则表达式是可以匹配文本片段的模式。最简单的正则表达式就是普通字符串，可以匹配其自身。换句话说，正则表达式&#39;python&#39;可以匹配字串&#39;python&#39;。你可以用这种匹配行为搜索文本中的模式，并且用计算后的值替换特定模式，或者将文本进行分段。 通配符&emsp;&emsp;正则表达式可以匹配多于一个的字符串，你可以使用一些特殊字符创建这类模式。比如点号.可以匹配任何字符（除了换行符），所以正则表达式&#39;.ython&#39;可以匹配字符串’python&#39;和&#39;jython&#39;。它还能匹配&#39;qython&#39;或者&#39; +ython&#39;（第一个字母是空格），但是不会匹配&#39;cpython&#39;或者&#39;ython&#39;这样的字符串，因为点号只能匹配一个字母，而不是两个或零个。因为它可以匹配“任何字符串”（除换行符外的任何单个字符），点号就称为通配符。 对特殊字符进行转义&emsp;&emsp;你需要知道，在正则表达式中如果将特殊字符作为普通字符使用会遇到问题，这很重要。比如，假设需要匹配字符串&#39;python.org&#39;，直接用&#39;python.org&#39;模式可以么？这么做是可以的，但是这样也会匹配’pythonzorg&#39;，这可不是所期望的结果（点号可以匹配除换行符外的任何字符，还记得吧？）。为了让特殊字符表现得像普通字符一样，需要对它进行转义，进行转义所做的在它前面加上反斜线。因此，在本例中可以使用&#39;python\\.org&#39;，这样就只会匹配’python.org&#39;了。 注意：为了获得re模块所需的单个反斜线，我们要在字符串中使用两个反斜线——为了通过解释器进行转义。这样就需要两个级别的转义了 ：（1）通过解释器转义；（2）通过re模块转义。（事实上，有些情况下可以使用单个反斜线，让解释器自动进行转义，但是别依赖这种功能。如果厌烦了使用双斜线，那么可以使用原始字符串，比如r&#39;python.org&#39;。） 字符集&emsp;&emsp;匹配任意字符可能很有用，但有些时候你需要更多的控制权。你可以使用中括号括住字符串来创建字符集。字符集可以匹配它所包括的任意字符，所以&#39;[pj]ython&#39;能够匹配&#39;python&#39;和&#39;jython&#39;，而非其他内容。你可以使用范围，比如&#39;[a-z]&#39;能够（按字母顺序）匹配a到z的任意一个字符，还可以通过一个接一个的方式将范围联合起来使用，比如&#39;[a-zA-Z0-9]&#39;能够匹配任意大小写字母和数字（注意字符集只能匹配一个这样的字符）。&emsp;&emsp;为了反转字符集，可以在开头使用^字符，比如&#39;[^abc]&#39;可以匹配任何除了a、b和c之外的字符。 选择符和子模式&emsp;&emsp;在字符串的每个字符都各不相同的情况下，字符集是很好用的，但如果只想匹配字符串&#39;python&#39;和&#39;perl&#39;呢？你就不能使用字符集或者通配符来指定某个特定的模式了。取而代之的是用于选择项的特殊字符：管道符|。因此，所需的模式可以写成&#39;python|perl&#39;。&emsp;&emsp;但是，有些时候不需要对整个模式使用选择运算符——只是模式的一部分。这时可以使用圆括号括起需要的部分，或称子模式。前例可以写成&#39;p(ython|erl)&#39;。（注意，术语子模式也适用于单个字符。） 可选项和重复子模式&emsp;&emsp;在子模式后面加上问号，它就变成了可选项。它可能出现在匹配字符串中，但并非必需的。例如，下面这个（稍微有点难懂的）模式：r&#39;(http://)?(www\.)?python\org&#39;只能匹配下列字符串（而不会匹配其他的）：&#39;http://www.python.org&#39;&#39;http://python.org&#39;&#39;www.python.org&#39;&#39;python.org&#39;对于上述例子，下面这些内容是值得注意的： 对点号进行了转义，防止它被作为通配符使用； 使用原始字符串减少所需反斜线的数量； 每个可选子模式都用圆括号括起； 可选子模式出现与否均可，而且互相独立。 问号表示子模式可以出现一次或者根本不出现。下面这些运算符允许子模式重复多次： (pattern)*：允许模式重复0次或多次； (pattern)+：允许模式重复1次或多次； (pattern){m,n}：允许模式重复m至n次。 例如，r&#39;w*\.python\.org&#39;，会匹配&#39;www.python.org&#39;，也会匹配&#39;.python.org&#39;、&#39;ww.python.org&#39;和&#39;wwwwwww.python.org&#39;。类似地，r&#39;w+\.python\.org&#39;匹配&#39;w.python.org&#39;但不匹配&#39;.python.org&#39;，而r&#39;w{3,4}\.python\.org&#39;只匹配&#39;www.python.org&#39;和&#39;wwww.python.org&#39;。 字符串的开始和结尾&emsp;&emsp;目前为止，所出现的模式匹配都是针对整个字符串的，但是也能寻找匹配模式的子字符串，比如字符串&#39;www.python.org&#39;中的子字符串&#39;www&#39;会能够匹配模式&#39;w+&#39;。在寻找这样的子字符串时，确定子字符串位于整个字符串的开始还是结尾是很有用的。比如，只想在字符串的开头而不是其他位置匹配&#39;ht+p&#39;，那么就可以使用脱字符^标记开始：&#39;^ht+p&#39;会匹配&#39;http://python.org&#39;(以及&#39;htttttttttp://python.org&#39;），但是不匹配&#39;www.python.org&#39;。类似地，字符串结尾用美元符号$标识。 注意：有关正则表达式运算符的完整列表，请参见Python类参考的4.2,1节的内容（http://python.org/doc/lib/re-syntax.html）。 参考表 &emsp;&emsp;如果不知道如何应用，只知道如何书写正则表达式还是不够的。re模块包含一些有用的操作正则表达式的函数。请移步Python正则表达式——re模块部分解析进一步了解。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>正则表达式</tag>
        <tag>字符串处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建博客——结束篇]]></title>
    <url>%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E2%80%94%E2%80%94%E7%BB%93%E6%9D%9F%E7%AF%87.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;作为本教程的结束篇，我也废话不多说，直接上折腾过程。 为站点添加腾讯公益404页面&emsp;&emsp;我们知道当访问网站中不存在的内容都会跳转到404页面，Hexo也提供了该功能。&emsp;&emsp;方法：在主题文件下的source文件夹创建404.html页面，html代码： 123456789101112131415161718&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 注意：在本地模式下调试不了该页面，要推送上GitHub才能看到该页面。如果说你推送上GitHub后还是调试不到该页面，其中一个原因可能是：没有绑定域名。没有绑定域名的话是无法创建404 页面，只会跳转到github默认404 page，绑定域名的话可使用腾讯公益404页面。还有一个注意的点是：http://github.io全部都是https，而js里面的链接有的是http，谷歌浏览器可能会拦截这个跳转，这个大家也要注意一下 绑定域名&emsp;&emsp;博客已经搭建好，能通过github的域名访问，但总归还是用自己的域名比较舒服。因此我们需要设置将自己的域名绑定到github这个博客项目上。&emsp;&emsp;去域名提供商那里注册个域名。至于去那个域名提供商博主就不推荐了，大家自个网上看。 在域名提供商那设置： 添加2条A记录：@—&gt;192.30.252.154@—&gt;192.30.252.153 添加一条CNAME记录：CNAME—&gt;XXXX.github.io 博客添加CNAME文件配置完域名解析后，进入博客目录，在source目录下新建CNAME文件（没有后缀名），写入域名，如：lyhub.xyz 运行：hexo g 运行：hexo d &emsp;&emsp;这个有些情况下需要等待时间才会生效，所以如果访问自个域名不成功的话不用急，多等一下也许就能上了。 SEO优化&emsp;&emsp;seo优化对于网站是否能被搜索引擎快速收录有很大帮助，因此适当做一些seo还是有必要的，可参考：http://www.arao.me/2015/hexo-next-theme-optimize-seo/ &emsp;&emsp;到这我们的 Hexo搭建博客 教程就告一段落了，以上也是博主搭建本博客的记录，在此分享给大家，希望能够帮助到各位看官。 参考https://thief.one/2017/03/03/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/http://www.arao.me/2015/hexo-next-theme-optimize-seo/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建博客——完善与美化]]></title>
    <url>%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E2%80%94%E2%80%94%E5%AE%8C%E5%96%84%E4%B8%8E%E7%BE%8E%E5%8C%96.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;完成上一篇文章的各个步骤操作之后我们已经有了一个基本的博客雏形，那么接下让我们一起来看看如何完善及美化这个雏形，从而得到一个成型的博客。 站点完善&emsp;&emsp;一般博客都会有 归档、分类、标签、搜索 这几个标配的功能，而且主题配置文件一般都有预设且默认开启这几个菜单项（看一下你现在的博客是不是有这几个菜单项，若没有则打开主题配置文件将对应菜单项前面的#号去掉，搜索这个菜单项先不用管它）。不过看一下你的博客的这几个选项，都没有正确的跳转。所以接下来我们来完善这一部分的内容。 添加「归档」页面归档页面已经默认为我们设置好了，所以这个不用管它。 添加「标签」页面&emsp;&emsp;新建「标签」页面，并在菜单中显示「标签」链接。「标签」页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。 底下代码是一篇包含标签的文章的例子： 123456---title: 标签测试文章tags: - Testing - Another Tag--- 请参阅 Hexo 的分类与标签文档，了解如何为文章添加标签或者分类。 新建页面在终端窗口下，定位到 blog 站点目录下。使用 hexo new page 新建一个页面，命名为 tags：hexo new page tags 设置页面类型打开source文件夹下的tags文件夹，编辑刚新建的页面，将页面的类型设置为 tags ，主题将自动为这个页面显示标签云。页面内容如下： 12345---title: 标签date: 2014-12-22 12:39:04type: &quot;tags&quot;--- 修改菜单（这一步如果在文章开头那已经操作了就不用）在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下: 1234menu: home: / archives: /archives tags: /tags 注意：如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 1234567禁用评论示例---title: 标签date: 2014-12-22 12:39:04type: &quot;tags&quot;comments: false--- 添加「分类」页面&emsp;&emsp;新建「分类」页面，并在菜单中显示「分类」链接。「分类」页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。 底下代码是一篇包含分类的文章的例子： 1234---title: 分类测试文章categories: Testing--- 请参阅 Hexo 的分类与标签文档，了解如何为文章添加标签或者分类。 新建页面在终端窗口下，定位到 blog 站点目录下。使用 hexo new page 新建一个页面，命名为 categories：hexo new page categories 设置页面类型打开source文件夹下的categories文件夹，编辑刚新建的页面，将页面的类型设置为 categories，主题将自动为这个页面显示分类。页面内容如下： 12345---title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;--- 修改菜单（这一步如果在文章开头那已经操作了就不用）在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下: 1234menu: home: / archives: /archives tags: /categories 注意：如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 1234567禁用评论示例---title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;comments: false--- &emsp;&emsp;现在已经完成了标签跟分类的设置，只要我们在写文章的时候按照Hexo 的分类与标签文档的规则为文章添加分类和标签，Hexo框架会自动帮我们处理。现在你可以多复制几篇文章，给它们打上不一样的分类和标签，然后重启服务，看看这两个菜单项的跳转是否正确。 配置「搜索」项&emsp;&emsp;Hexo下有几种搜索方式，本博客用的是Hexo的Local Search。&emsp;&emsp;Hexo提供的Local Search,原理是通过hexo-generator-search插件在本地生成一个search.xml文件，搜索的时候从这个文件中根据关键字检索出相应的链接。安装步骤 安装 hexo-generator-search在站点的根目录下执行以下命令：npm install hexo-generator-search --save 安装 hexo-generator-searchdb在站点的根目录下执行以下命令：npm install hexo-generator-searchdb --save 启用搜索编辑站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑 主题配置文件，找到Local search条目：将Local search条目下的enable对应的值设置为true &emsp;&emsp;保存并重启服务你会发现菜单栏多了一个搜索项，点击它就可以进行站内搜索。&emsp;&emsp;至于其它的搜索方式，大家可以上网搜教程，这里就不再赘述。然后，看到这相信小伙伴也知道了对于菜单栏，我们还可以自定义一些自己想要的项，这个我在这也不赘述ಠᴗಠ，大家自个找找教程琢磨琢磨。 评论和分享评论&emsp;&emsp;next已经预设好了很多第三方评论，在主题配置文件了搜索Third Party Services Settings，下面有很多第三方评论。&emsp;&emsp;就本站而言，用的是友言。 首先去友言官网注册个账号，在获取代码那里找到你的uid。 在主题配置文件中找到youyan条目，删除youyan_uid前面的注释，将其值改成你的uid 分享&emsp;&emsp;友言评论也配套了jiathis分享在主题配置文件中搜索并将jiathis条目修改成如下 123jiathis: uid: 你的uidadd_this_id: 你的uid OK，保存，重启服务，打开一篇文章，在文章的底部可以看到分享按钮和评论框了。 主题美化文章内插入图片在文章中写入: 1![](/upload_image/1.jpg) 然后进入themes-主题名-source-upload_image目录下(自己创建)，将图片放到这个目录下，就可以了。 说明：当执行hexo g命令时，会自动把图片复制到 public文件的upload_image目录下。 文章中添加居中引用模块文章Markdown中填写如下：&lt;blockquote class=&quot;blockquote-center&quot;&gt;XXXXXXXXXXXXXXXXXX&lt;/blockquote&gt; 在文章底部增加版权信息在目录 hexo-theme-next/layout/_macro/下添加 my-copyright.swig： 12345678910111213141516171819202122232425262728293031&#123;% if page.copyright %&#125;&lt;div class=&quot;my_post_copyright&quot;&gt; &lt;script src=&quot;//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css&quot;&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot;&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=&quot;/&quot; title=&quot;访问 &#123;&#123; theme.author &#125;&#125; 的个人博客&quot;&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title=&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=&quot;copy-path&quot; title=&quot;点击复制文章链接&quot;&gt;&lt;i class=&quot;fa fa-clipboard&quot; data-clipboard-text=&quot;&#123;&#123; page.permalink &#125;&#125;&quot; aria-label=&quot;复制成功！&quot;&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=&quot;fa fa-creative-commons&quot;&gt;&lt;/i&gt; &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; target=&quot;_blank&quot; title=&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard(&apos;.fa-clipboard&apos;); clipboard.on(&apos;success&apos;, $(function()&#123; $(&quot;.fa-clipboard&quot;).click(function()&#123; swal(&#123; title: &quot;&quot;, text: &apos;复制成功&apos;, html: false, timer: 500, showConfirmButton: false &#125;); &#125;); &#125;)); &lt;/script&gt;&#123;% endif %&#125; 在目录hexo-theme-next/source/css/_common/components/post/下添加my-post-copyright.styl： 123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 修改hexo-theme-next/layout/_macro/post.swig，在代码 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;wechat-subscriber.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 之前添加增加如下代码： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;my-copyright.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 修改hexo-theme-next/source/css/_common/components/post/post.styl文件，在最后一行增加代码： 1@import &quot;my-post-copyright&quot; 如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加copyright: true的设置，类似： 1234567---title: date: tags: categories: copyright: true--- 自定义hexo new生成md文件的选项在/scaffolds/post.md文件中添加： 12345678910---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:categories: copyright: truepermalink: 01top: 0password:--- 添加顶部加载条打开/themes/hexo-theme-next/layout/_partials/head.swig文件，在maximum-scale=1”/&gt;后添加如下代码: 12&lt;script src=&quot;//cdn.bootcss.com/pace/1.0.2/pace.min.js&quot;&gt;&lt;/script&gt;&lt;link href=&quot;//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css&quot; rel=&quot;stylesheet&quot;&gt; 但是，默认的是粉色的，要改变颜色可以在/themes/next/layout/_partials/head.swig文件中添加如下代码（接在刚才link的后面） 12345678910111213&lt;style&gt; .pace .pace-progress &#123; background: #1E92FB; /*进度条颜色*/ height: 3px; &#125; .pace .pace-progress-inner &#123; box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/ &#125; .pace .pace-activity &#123; border-top-color: #1E92FB; /*上边框颜色*/ border-left-color: #1E92FB; /*左边框颜色*/ &#125;&lt;/style&gt; 主页文章添加阴影效果打开\themes\hexo-theme-next\source\css_custom\custom.styl,向里面加入： 12345678// 主页文章添加阴影效果 .post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); &#125; 修改文章底部的那个带#号的标签&emsp;&emsp;修改模板/themes/hexo-theme-next/layout/_macro/post.swig，搜索 rel=”tag”&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 修改文章内链接文本样式&emsp;&emsp;将链接文本设置为蓝色，鼠标划过时文字颜色加深，并显示下划线。&emsp;&emsp;找到文件 themes\hexo-theme-next\source\css_custom\custom.styl ，添加如下 css 样式： 12345678.post-body p a &#123; color: #0593d3; border-bottom: none; &amp;:hover &#123; color: #0477ab; text-decoration: underline; &#125;&#125; 设置侧边栏头像&emsp;&emsp;首先将你的头像图片放到主题文件夹下的source的images文件夹下&emsp;&emsp;在主题配置文件中找到Sidebar Avatar条目，将该条目下的avatar值修改成/images/xxxx.jpg 侧边栏RSS订阅设置在站点根目录下运行：npm install --save hexo-generator-feed在站点配置文件里添加 123# Extensions## Plugins: http://hexo.io/plugins/plugins: hexo-generate-feed 在主题配置文件里找到rss条目，修改成 1234# Set rss to false to disable feed link.# Leave rss as empty to use site&apos;s feed link.# Set rss to specific value if you have burned your feed already.rss: /atom. 配置完之后运行：hexo g 在每篇文章末尾统一添加“本文结束”标记具体实现方法：在路径 \themes\hexo-theme-next\layout_macro 中新建 passage-end-tag.swig 文件,并添加以下内容： 12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束，感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 接着打开\themes\next\layout_macro\post.swig文件，在post-body 之后， post-footer 之前添加如下代码（post-footer之前两个DIV）： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;passage-end-tag.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 然后打开主题配置文件（_config.yml),在末尾添加： 123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 定制Hexo-Next底部logo栏http://www.jianshu.com/p/4fbc57269f1b 设置「阅读全文」&amp; 设置页面文章的篇数 Next常见问题：http://theme-next.iissnan.com/faqs.html 提醒：在更新博客内容时，最好先在本地调试完毕后（hexo server），再推送到github上。 &emsp;&emsp;至此，我们的博客算是真正意义上的成型了。如果说你比较熟悉前端的知识，那你可以进一步折腾博客的布局、配色、UI图标等等，深度定制个人专属博客。或者你想为博客新添加页面，菜单项的，网上也有很多教程可以看。在下面的参考链接中有很多有趣的东西给我们的博客增色，大家可以去瞧瞧。&emsp;&emsp;我们的教程也参不多可以结束了，不过由于篇幅的原因，另外几个关于站点的完善我放在另一篇文章里，Hexo搭建博客——结束篇，有兴趣的同学可以继续折腾折腾。 参考https://thief.one/2017/03/03/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/http://www.jianshu.com/p/4fbc57269f1bhttp://www.jianshu.com/p/f054333ac9e6]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建博客]]></title>
    <url>%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;Hexo，就同官网上的标题写着：A fast, simple &amp; powerful blog framework 。的确，在博主开始搭建本博客到配置细节到博客上线，确实是体验到fast、simple、powerful，简简单单几个语句就搭好了整个博客的基本框架。网上有关于Hexo的教程也很多，遇到什么问题一搜就能解决。官方给出的文档也很容易看，配合各种主题，插件等搭建一个炫酷的、高定制的个人博客完全不在话下。 必要软件的安装 下载安装git 下载node.js并安装（默认会安装npm） 下载安装hexo。方法：打开cmd 运行 npm install hexo-cli -g 搭建本地博客&emsp;&emsp;按照Hexo官网的方法，cmd下切换到你想要放置博客的目录下，运行以下命令： 1234hexo init blog # 生成blog文件夹，博客所有的东西都放在该目录下cd blognpm installhexo server # 运行程序，访问本地localhost:4000可以看到博客已经搭建成功 注意：在运行hexo server之前最好先运行netstat -ano查看 4000 端口是否被占用。若被占用了改用hexo s -p xxxx，xxxx是端口号，比如可以用hexo s -p 8000，之后 访问本地 localhost:8000 可以看到博客已经搭建成功。 将本地博客和GitHub关联&emsp;&emsp;在Github上创建名字为XXXX.github.io的项目，XXXX为自己的github用户名。打开本地的blog文件夹项目内的_config.yml配置文件，将其中的type设置为git 。 deploy: type: git repository: https://github.com/XXX/XXX.github.io.git branch: master 运行：npm install hexo-deployer-git –save运行：hexo g（本地生成静态文件）运行：hexo d（将本地静态文件推送至Github）之后，打开浏览器，访问http://XXXX.github.io 更新博客文章&emsp;&emsp;在blog目录下执行：hexo new &quot;第一篇文章&quot;，会在source-&gt;_posts文件夹内生成一个.md文件。 编辑该文件（遵循Markdown规则） 修改起始字段（基本不用修改，后续会讲到这些条目） title 文章的标题 date 创建日期 （文件的创建日期 ） updated 修改日期 （ 文件的修改日期） comments 是否开启评论 true tags 标签 categories 分类 permalink url中的名字（文件名） 编写正文内容（遵循Markdown规则） 运行hexo g 生成本地静态文件（Public目录） 运行hexo deploy 将本地静态文件推送至github（hexo d） 访问http://XXXX.github.io，看看成果 &emsp;&emsp;至此，我相信你已经大概清楚了Hexo框架如何写文章以及更新GitHub仓库的博客文章。&emsp;&emsp;接下来是一小部分关于Hexo一些完善操作和个性化定制内容。 个性化定制定制基本信息&emsp;&emsp;在根目录下的_config.yml（站点配置文件）文件中，可以修改标题，作者等信息。打开编辑该文件，注意：每一个值的冒号后面都有一个半角空格！ 未生效的写法：title:nMask的博客能生效的写法：title:[空格]nMask的博客 里面还有其他的条目等你熟悉这个框架后再慢慢去琢磨。 主题&emsp;&emsp;Hexo下的主题有很多，网上搜一下一大把。&emsp;&emsp;cmd进入blog下的themes目录，下载主题 (以next主题为例)： git clone https://github.com/iissnan/hexo-theme-next.git &emsp;&emsp;打开blog的__config.yml（站点配置文件）文件，将themes修改为hexo-theme-next（下载到的主题文件夹的名字）运行：hexo g 和hexo d访问http://XXXX.github.io，可以看到网页主题已经发生了改变。 关于hexo-theme-next主题下的一些个性化配置，参考：Next主题配置 设置主题语言&emsp;&emsp;大部分主题默认是使用英语的，如何修改成自己需要的语言。&emsp;&emsp;每一个主题文件夹下都有一个langues文件夹，里面存放着该主题支持的语言。打开这些文件你可以发现其实这里面都是对主题配置文件里的条目的对应翻译（因此如果对某个翻译不满意，你自己可以进行修改）。复制一个你需要的语言的文件名（不包括后缀名），比如zh-Hans，然后打开blog下的_config_yml（站点配置文件）文件，找到language条目，将其对应的值修改成zh-Hans。保存，重启服务就可以在本地看到效果，之后再推送到GitHub上（hexo g ， hexo d） 添加或删除菜单项&emsp;&emsp;打开blog的theme目录，进入hexo-theme-next目录，编辑_config_yml（主题配置文件）文件，找到menu:字段，在该字段下有几个预设菜单项，有的用#注释掉。你可以根据自己的需求进行调整。 &emsp;&emsp;经过这一小部分的调整，一个真正属于你的博客的雏形算是基本形成了。 &emsp;&emsp;至此，我想你多多少少还是有点成就感吧。不错，稍微享受一下之后，接下来还要继续完善这个博客，包括菜单选项的跳转以及主题的美化。 请看Hexo搭建博客——完善与美化 参考https://thief.one/2017/03/03/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开启Hexo博客之旅]]></title>
    <url>%2F%E5%BC%80%E5%90%AFHexo%E5%8D%9A%E5%AE%A2%E4%B9%8B%E6%97%85.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;本博客用的是Hexo+Next主题+Github部署，具体可参考: Hexo搭建博客 &emsp;&emsp;本博客主要用于记录及分享个人技术总结以及心得，后续或许有其他非技术方面的内容分享，若有合作或者疑问可发送邮件到：liyonggdut@163.com&emsp;&emsp;关于友情链接，可在下方留言，欢迎互相添加友情链接。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>原创</tag>
      </tags>
  </entry>
</search>
